{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParsXML(path):\n",
    "    data = []\n",
    "    filesNames = os.listdir(path)\n",
    "    for fileName in filesNames:\n",
    "        tree = ET.parse(path + \"/\" + fileName)\n",
    "        root = tree.getroot()\n",
    "        headline = tree.find('headline').text\n",
    "        text = list(tree.find('text'))\n",
    "        content = \"\"\n",
    "        topics = []\n",
    "        for elem in text:\n",
    "            content += elem.text\n",
    "        bip_topics = []\n",
    "        dc_date_published = \"\"\n",
    "        itemid = root.attrib['itemid']\n",
    "        XMLfilename = fileName\n",
    "        for node in root.iter():\n",
    "            if node.tag == 'dc' and node.attrib['element'] == \"dc.date.published\":\n",
    "                dc_date_published = node.attrib['value']\n",
    "            if node.tag == 'codes' and node.attrib['class'] == \"bip:topics:1.0\":\n",
    "                topics = list(node)\n",
    "                for topic in topics:\n",
    "                    bip_topics.append(topic.attrib['code'])\n",
    "\n",
    "        if len(bip_topics) !=0:\n",
    "            data.append([headline, content, bip_topics, dc_date_published, itemid,\n",
    "                 XMLfilename])\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(ParsXML(\"Data\"), columns = ['headline', 'text', \n",
    "                                   'bip:topics', 'dc.date.published',\n",
    "                                   'itemid', 'XMLfilename'])\n",
    "\n",
    "df.to_csv(\"DataMultilabelRaw.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DataMultilabelRaw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df[['text', 'bip:topics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(dataframe):\n",
    "    ps = PorterStemmer()\n",
    "    for i in range(0, len(dataframe)):\n",
    "        content = dataframe.iat[i, 0]\n",
    "        temp = re.sub('[^a-zA-Z]', ' ', content)\n",
    "        temp = temp.lower()\n",
    "        temp = temp.split()\n",
    "        temp = [ps.stem(word) for word in temp if not word in set(stopwords.words('english'))]\n",
    "        content = ' '.join(temp)\n",
    "        dataframe.iloc[i, 0] = content\n",
    "    return dataframe\n",
    "dataset = cleanText(dataset)\n",
    "dataset.to_csv(\"DataMultilabelCleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"DataMultilabelCleaned.csv\")\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "cleanedString = \"\"\n",
    "labels = []\n",
    "for i in range(0,len(y)):\n",
    "    cleanedString = re.sub('\\W+',' ', y[i])\n",
    "    labels.append(cleanedString.split())\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i in range(0,len(X)):\n",
    "    documents.append(X[i, 0].split())\n",
    "def doc2Vec(documents):    \n",
    "    docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(documents)]\n",
    "    model = Doc2Vec(docs, vector_size=100, window=10, min_count=1, workers=8)\n",
    "    return model\n",
    "\n",
    "model = doc2Vec(X)\n",
    "dependentVariables = []\n",
    "for i in range(0,len(documents)):\n",
    "    dependentVariables.append(model.infer_vector(documents[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(dependentVariables, (len(dataset), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/pkg/python/root-python-2.7/lib/python2.7/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=100, units=500)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/local/pkg/python/root-python-2.7/lib/python2.7/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=102, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33779 samples, validate on 14478 samples\n",
      "Epoch 1/2\n",
      "33779/33779 [==============================] - 96s 3ms/step - loss: 0.1171 - acc: 0.9680 - val_loss: 0.1075 - val_acc: 0.9682\n",
      "Epoch 2/2\n",
      "33779/33779 [==============================] - 92s 3ms/step - loss: 0.1080 - acc: 0.9682 - val_loss: 0.1075 - val_acc: 0.9677\n",
      "Epoch 1/1\n",
      "33779/33779 [==============================] - 30s 877us/step - loss: 0.1354 - acc: 0.9677\n",
      "Epoch 1/2\n",
      "33779/33779 [==============================] - 43s 1ms/step - loss: 0.1260 - acc: 0.9678: 0s - loss: 0.1263 - a\n",
      "Epoch 2/2\n",
      "33779/33779 [==============================] - 44s 1ms/step - loss: 0.1079 - acc: 0.9682\n",
      "0.9682222098150044\n",
      "{'epochs': 2, 'batch_size': 20}\n"
     ]
    }
   ],
   "source": [
    "def model_creator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim=500, init='uniform', activation='relu', input_dim=100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_dim=102, init='uniform', activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "classifier = KerasClassifier(build_fn=model_creator, epochs=2, verbose=1, batch_size=10, validation_data=(X_test, y_test))\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "classifier = KerasClassifier(build_fn=model_creator)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "batch_sizes = [10, 20 , 30]\n",
    "epochs = [2, 3]\n",
    "parameters = [{'batch_size': batch_sizes, 'epochs': epochs}]\n",
    "grid_search = GridSearchCV(classifier, parameters, n_jobs=-1, cv=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/pkg/python/root-python-2.7/lib/python2.7/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=100, units=500)`\n",
      "  \"\"\"\n",
      "/local/pkg/python/root-python-2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=102, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "43431/43431 [==============================] - 109s 3ms/step - loss: 0.1150 - acc: 0.9680\n",
      "Epoch 2/2\n",
      "43431/43431 [==============================] - 113s 3ms/step - loss: 0.1080 - acc: 0.9681\n",
      "4826/4826 [==============================] - 2s 311us/step\n",
      "acc: 96.83%\n",
      "Epoch 1/2\n",
      "43431/43431 [==============================] - 108s 2ms/step - loss: 0.1153 - acc: 0.9680\n",
      "Epoch 2/2\n",
      "43431/43431 [==============================] - 112s 3ms/step - loss: 0.1080 - acc: 0.9681\n",
      "4826/4826 [==============================] - 1s 285us/step\n",
      "acc: 96.86%\n",
      "Epoch 1/2\n",
      "43431/43431 [==============================] - 113s 3ms/step - loss: 0.1149 - acc: 0.9680\n",
      "Epoch 2/2\n",
      "43431/43431 [==============================] - 112s 3ms/step - loss: 0.1079 - acc: 0.9681\n",
      "4826/4826 [==============================] - 2s 389us/step\n",
      "acc: 96.83%\n",
      "Epoch 1/2\n",
      "43431/43431 [==============================] - 116s 3ms/step - loss: 0.1149 - acc: 0.9680\n",
      "Epoch 2/2\n",
      "43431/43431 [==============================] - 115s 3ms/step - loss: 0.1079 - acc: 0.9681\n",
      "4826/4826 [==============================] - 2s 329us/step\n",
      "acc: 96.82%\n",
      "Epoch 1/2\n",
      "43431/43431 [==============================] - 117s 3ms/step - loss: 0.1151 - acc: 0.9680\n",
      "Epoch 2/2\n",
      "43431/43431 [==============================] - 116s 3ms/step - loss: 0.1080 - acc: 0.9681\n",
      "4826/4826 [==============================] - 2s 348us/step\n",
      "acc: 96.83%\n",
      "Epoch 1/2\n",
      "43431/43431 [==============================] - 111s 3ms/step - loss: 0.1151 - acc: 0.9680\n",
      "Epoch 2/2\n",
      "43431/43431 [==============================] - 114s 3ms/step - loss: 0.1079 - acc: 0.9682\n",
      "4826/4826 [==============================] - 2s 345us/step\n",
      "acc: 96.81%\n",
      "Epoch 1/2\n",
      "43431/43431 [==============================] - 114s 3ms/step - loss: 0.1149 - acc: 0.9681\n",
      "Epoch 2/2\n",
      "43431/43431 [==============================] - 118s 3ms/step - loss: 0.1078 - acc: 0.9682\n",
      "4826/4826 [==============================] - 2s 319us/step\n",
      "acc: 96.80%\n",
      "Epoch 1/2\n",
      "43432/43432 [==============================] - 113s 3ms/step - loss: 0.1149 - acc: 0.9681\n",
      "Epoch 2/2\n",
      "43432/43432 [==============================] - 119s 3ms/step - loss: 0.1079 - acc: 0.9681\n",
      "4825/4825 [==============================] - 2s 372us/step\n",
      "acc: 96.80%\n",
      "Epoch 1/2\n",
      "43432/43432 [==============================] - 119s 3ms/step - loss: 0.1151 - acc: 0.9680 1\n",
      "Epoch 2/2\n",
      "43432/43432 [==============================] - 118s 3ms/step - loss: 0.1080 - acc: 0.9682 0s - loss: 0.1080\n",
      "4825/4825 [==============================] - 2s 391us/step\n",
      "acc: 96.82%\n",
      "Epoch 1/2\n",
      "43432/43432 [==============================] - 120s 3ms/step - loss: 0.1150 - acc: 0.9680\n",
      "Epoch 2/2\n",
      "43432/43432 [==============================] - 117s 3ms/step - loss: 0.1079 - acc: 0.9682\n",
      "4825/4825 [==============================] - 2s 399us/step\n",
      "acc: 96.81%\n",
      "96.82% (+/- 0.02%)\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "cvscores = []\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim=500, init='uniform', activation='relu', input_dim=100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_dim=102, init='uniform', activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X[train_index], y[train_index], epochs=2, batch_size=10, verbose=1)\n",
    "    scores = model.evaluate(X[test_index], y[test_index], verbose=1)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
