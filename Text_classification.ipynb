{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.models import Sequential\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import stopwords\n",
    "from keras import utils\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParsXML(path):\n",
    "    data = []\n",
    "    filesNames = os.listdir(path)\n",
    "    for fileName in filesNames:\n",
    "        tree = ET.parse(path + \"/\" + fileName)\n",
    "        root = tree.getroot()\n",
    "        headline = tree.find('headline').text\n",
    "        text = list(tree.find('text'))\n",
    "        content = \"\"\n",
    "        topics = []\n",
    "        for elem in text:\n",
    "            content += elem.text\n",
    "        bip_topics = []\n",
    "        dc_date_published = \"\"\n",
    "        itemid = root.attrib['itemid']\n",
    "        XMLfilename = fileName\n",
    "        for node in root.iter():\n",
    "            if node.tag == 'dc' and node.attrib['element'] == \"dc.date.published\":\n",
    "                dc_date_published = node.attrib['value']\n",
    "            if node.tag == 'codes' and node.attrib['class'] == \"bip:topics:1.0\":\n",
    "                topics = list(node)\n",
    "                for topic in topics:\n",
    "                    bip_topics.append(topic.attrib['code'])\n",
    "\n",
    "        if len(bip_topics) !=0:\n",
    "            data.append([headline, content, bip_topics[0], dc_date_published, itemid,\n",
    "                 XMLfilename])\n",
    "    \n",
    "    return data\n",
    "\n",
    "df = pd.DataFrame(ParsXML(\"Data\"), columns = ['headline', 'text', \n",
    "                                   'bip:topics', 'dc.date.published',\n",
    "                                   'itemid', 'XMLfilename'])\n",
    "\n",
    "df.to_csv(\"DataRaw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DataRaw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df[['text', 'bip:topics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48257 [00:00<?, ?it/s]/local/pkg/python/root-python-2.7-a5/lib/python2.7/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/local/pkg/python/root-python-2.7/lib/python2.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "100%|██████████| 48257/48257 [2:17:59<00:00,  5.70it/s]  \n"
     ]
    }
   ],
   "source": [
    "def cleanText(dataframe):\n",
    "    for i in tqdm(range(0, len(dataframe))):\n",
    "        content = dataframe.iat[i, 0]\n",
    "        temp = re.sub('[^a-zA-Z]', ' ', content)\n",
    "        temp = temp.lower()\n",
    "        temp = temp.split()\n",
    "        temp = [word for word in temp if not word in set(stopwords.words('english'))]\n",
    "        content = ' '.join(temp)\n",
    "        dataframe.iloc[i, 0] = content\n",
    "    return dataframe\n",
    "dataset = cleanText(dataset)\n",
    "dataset.to_csv(\"DataCleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"DataCleaned.csv\")\n",
    "frequency = dataset['bip:topics'].value_counts()\n",
    "for k in frequency.keys():\n",
    "    if frequency.get(k) < 20:\n",
    "        dataset = dataset[dataset['bip:topics'] != k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "with open(\"glove.6B.100d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i in range(0, len(dataset)):\n",
    "    documents.append(dataset.iat[i, 0].split())\n",
    "\n",
    "vector = np.zeros(100)\n",
    "final = np.zeros(100)\n",
    "docs = []\n",
    "for i in range(0,len(documents)):\n",
    "    for j in range(0,len(documents[i])):\n",
    "        if documents[i][j] in embeddings_dict:\n",
    "            vector = embeddings_dict[documents[i][j]]\n",
    "        else:\n",
    "            vector = np.zeros(100)\n",
    "        final = np.add(final, vector)\n",
    "    final = np.true_divide(final, len(documents[i]))\n",
    "    docs.append(final)\n",
    "    final = np.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(docs, (len(dataset), 100))\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelEncoder = LabelEncoder()\n",
    "y = labelEncoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "num_classes = len(dataset['bip:topics'].unique())\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim=1000, init='uniform', activation='relu', input_dim=100))\n",
    "    model.add(Dense(output_dim=1000, init='uniform', activation='relu'))\n",
    "    model.add(Dense(output_dim=1000, init='uniform', activation='relu'))\n",
    "    model.add(Dense(output_dim=num_classes, init='uniform', activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "classifier = KerasClassifier(build_fn=model_creator, epochs=2, verbose=1, batch_size=10, validation_split=0.1)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier = KerasClassifier(build_fn=model_creator)\n",
    "batch_sizes = [10, 20, 100]\n",
    "epochs = [3, 10]\n",
    "parameters = [{'batch_size': batch_sizes, 'epochs': epochs}]\n",
    "grid_search = GridSearchCV(classifier, parameters, n_jobs=-1, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/pkg/python/root-python-2.7/lib/python2.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=100, units=1000)`\n",
      "  \n",
      "/local/pkg/python/root-python-2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1000, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "/local/pkg/python/root-python-2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1000, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/local/pkg/python/root-python-2.7/lib/python2.7/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=38, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "33645/33645 [==============================] - 41s 1ms/step - loss: 1.3395 - acc: 0.6182\n",
      "Epoch 2/10\n",
      "33645/33645 [==============================] - 42s 1ms/step - loss: 0.9401 - acc: 0.7180\n",
      "Epoch 3/10\n",
      "33645/33645 [==============================] - 41s 1ms/step - loss: 0.8360 - acc: 0.7459\n",
      "Epoch 4/10\n",
      "33645/33645 [==============================] - 41s 1ms/step - loss: 0.7702 - acc: 0.7620\n",
      "Epoch 5/10\n",
      "33645/33645 [==============================] - 41s 1ms/step - loss: 0.7213 - acc: 0.7759\n",
      "Epoch 6/10\n",
      "33645/33645 [==============================] - 42s 1ms/step - loss: 0.6718 - acc: 0.7849\n",
      "Epoch 7/10\n",
      "33645/33645 [==============================] - 41s 1ms/step - loss: 0.6320 - acc: 0.7978\n",
      "Epoch 8/10\n",
      "33645/33645 [==============================] - 41s 1ms/step - loss: 0.5917 - acc: 0.8057\n",
      "Epoch 9/10\n",
      "33645/33645 [==============================] - 41s 1ms/step - loss: 0.5581 - acc: 0.8155\n",
      "Epoch 10/10\n",
      "33645/33645 [==============================] - 42s 1ms/step - loss: 0.5191 - acc: 0.8245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c35c89410>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(output_dim=1000, init='uniform', activation='relu', input_dim=100))\n",
    "model.add(Dense(output_dim=1000, init='uniform', activation='relu'))\n",
    "model.add(Dense(output_dim=1000, init='uniform', activation='relu'))\n",
    "model.add(Dense(output_dim=num_classes, init='uniform', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "cvscores = []\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim=500, init='uniform', activation='relu', input_dim=100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_dim=102, init='uniform', activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X[train_index], y[train_index], epochs=2, batch_size=10, verbose=1)\n",
    "    scores = model.evaluate(X[test_index], y[test_index], verbose=1)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
